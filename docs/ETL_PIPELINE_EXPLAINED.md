# Giáº£i thÃ­ch ETL Pipeline - etl_pipeline.py

## ğŸ¯ Má»¥c Ä‘Ã­ch cá»§a Pipeline

Pipeline nÃ y Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n: **PhÃ¢n tÃ­ch dá»¯ liá»‡u bÃ¡n hÃ ng tá»« SQL Server vÃ  táº¡o cÃ¡c bÃ¡o cÃ¡o analytics**

### Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t:
1. **Data náº±m ráº£i rÃ¡c**: Dá»¯ liá»‡u bÃ¡n hÃ ng náº±m trong nhiá»u báº£ng khÃ¡c nhau trong SQL Server
   - `SalesOrderHeader` - ThÃ´ng tin Ä‘Æ¡n hÃ ng
   - `SalesOrderDetail` - Chi tiáº¿t tá»«ng sáº£n pháº©m trong Ä‘Æ¡n hÃ ng
   - `Product` - ThÃ´ng tin sáº£n pháº©m
   - `ProductCategory`, `ProductSubcategory` - PhÃ¢n loáº¡i sáº£n pháº©m

2. **Cáº§n analytics**: Muá»‘n biáº¿t:
   - Doanh thu theo nÄƒm nhÆ° tháº¿ nÃ o?
   - Sáº£n pháº©m nÃ o bÃ¡n cháº¡y nháº¥t?
   - Doanh thu theo tá»«ng category nhÆ° tháº¿ nÃ o?

3. **Performance**: SQL Server khÃ´ng phÃ¹ há»£p Ä‘á»ƒ cháº¡y analytics queries phá»©c táº¡p trÃªn dá»¯ liá»‡u lá»›n

### Giáº£i phÃ¡p:
Pipeline ETL nÃ y sáº½:
- **Extract**: Láº¥y dá»¯ liá»‡u tá»« SQL Server
- **Transform**: Join cÃ¡c báº£ng, tÃ­nh toÃ¡n metrics
- **Load**: LÆ°u vÃ o Data Lake (Parquet format) Ä‘á»ƒ:
  - Tá»‘c Ä‘á»™ Ä‘á»c nhanh hÆ¡n
  - CÃ³ thá»ƒ query láº¡i nhiá»u láº§n
  - PhÃ¹ há»£p cho analytics

---

## ğŸ“Š Flow Tá»•ng Quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL PIPELINE FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRACT (TrÃ­ch xuáº¥t)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ SQL Server Database                 â”‚
   â”‚ â”œâ”€â”€ SalesOrderHeader                â”‚
   â”‚ â”œâ”€â”€ SalesOrderDetail                â”‚
   â”‚ â”œâ”€â”€ Product                         â”‚
   â”‚ â”œâ”€â”€ ProductSubcategory              â”‚
   â”‚ â””â”€â”€ ProductCategory                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ JDBC Connection
                  â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Spark DataFrames                    â”‚
   â”‚ (In-memory distributed data)        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. VALIDATE (Kiá»ƒm tra cháº¥t lÆ°á»£ng)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ - Check row count                   â”‚
   â”‚ - Check null values                 â”‚
   â”‚ - Validate data integrity           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. TRANSFORM (Biáº¿n Ä‘á»•i)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Join Tables                         â”‚
   â”‚ â”œâ”€â”€ OrderDetail + OrderHeader       â”‚
   â”‚ â”œâ”€â”€ + Product                       â”‚
   â”‚ â”œâ”€â”€ + Subcategory                   â”‚
   â”‚ â””â”€â”€ + Category                      â”‚
   â”‚                                     â”‚
   â”‚ Calculate Metrics                   â”‚
   â”‚ â”œâ”€â”€ Sales by Year                   â”‚
   â”‚ â”œâ”€â”€ Top Products                    â”‚
   â”‚ â””â”€â”€ Sales by Category & Year        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. LOAD (Táº£i dá»¯ liá»‡u)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Data Lake (Parquet Format)          â”‚
   â”‚ â”œâ”€â”€ /adw/analytics/                 â”‚
   â”‚ â”‚   â”œâ”€â”€ sales_by_year               â”‚
   â”‚ â”‚   â”œâ”€â”€ top_products                â”‚
   â”‚ â”‚   â””â”€â”€ sales_by_category_year      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Giáº£i thÃ­ch Chi Tiáº¿t Tá»«ng Function

### 1. Function `extract_data()`

**Má»¥c Ä‘Ã­ch**: Láº¥y dá»¯ liá»‡u tá»« SQL Server vÃ o Spark DataFrames

**CÃ¡ch hoáº¡t Ä‘á»™ng**:

```python
def extract_data(spark: SparkSession, config: dict, logger):
```

**Input**:
- `spark`: SparkSession Ä‘á»ƒ káº¿t ná»‘i Spark
- `config`: Dictionary chá»©a cáº¥u hÃ¬nh (tá»« config.yaml)
- `logger`: Logger Ä‘á»ƒ ghi log

**Quy trÃ¬nh**:

1. **Láº¥y thÃ´ng tin káº¿t ná»‘i**:
   ```python
   jdbc_url = get_jdbc_url(config)  # "jdbc:sqlserver://localhost:1433..."
   props = get_jdbc_properties(config)  # username, password, driver
   ```

2. **Äá»c danh sÃ¡ch báº£ng cáº§n extract** tá»« config:
   ```yaml
   tables:
     sales:
       - "Sales.SalesOrderHeader"
       - "Sales.SalesOrderDetail"
     production:
       - "Production.Product"
       - "Production.ProductSubcategory"
       - "Production.ProductCategory"
   ```

3. **Loop qua tá»«ng báº£ng vÃ  Ä‘á»c**:
   ```python
   for table in tables_config.get('sales', []):
       df = spark.read.jdbc(jdbc_url, table, properties=props)
       # Äá»c tá»« SQL Server qua JDBC
   ```

4. **LÆ°u vÃ o dictionary**:
   ```python
   dataframes['salesorderheader'] = df
   dataframes['salesorderdetail'] = df
   # ...
   ```

**Output**: Dictionary chá»©a cÃ¡c Spark DataFrames

**VÃ­ dá»¥ káº¿t quáº£**:
```python
{
    'salesorderheader': DataFrame[SalesOrderID, OrderDate, CustomerID, ...],
    'salesorderdetail': DataFrame[SalesOrderDetailID, SalesOrderID, ProductID, ...],
    'product': DataFrame[ProductID, Name, ProductSubcategoryID, ...],
    ...
}
```

---

### 2. Function `validate_data()`

**Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u sau khi extract

**CÃ¡ch hoáº¡t Ä‘á»™ng**:

```python
def validate_data(dataframes: dict, logger):
```

**Kiá»ƒm tra**:

1. **Row count**: Äáº£m báº£o báº£ng khÃ´ng rá»—ng
   ```python
   row_count = df.count()
   if row_count == 0:
       logger.warning("Table is empty!")
   ```

2. **Null values**: Kiá»ƒm tra cÃ¡c cá»™t quan trá»ng cÃ³ null khÃ´ng
   ```python
   for col_name in df.columns[:5]:  # Check 5 cá»™t Ä‘áº§u
       null_count = df.filter(col(col_name).isNull()).count()
   ```

**Táº¡i sao cáº§n validate?**
- PhÃ¡t hiá»‡n sá»›m lá»—i dá»¯ liá»‡u
- Äáº£m báº£o pipeline khÃ´ng cháº¡y vá»›i dá»¯ liá»‡u sai
- GiÃºp debug dá»… hÆ¡n

---

### 3. Function `transform_data()`

**Má»¥c Ä‘Ã­ch**: Join cÃ¡c báº£ng vÃ  tÃ­nh toÃ¡n cÃ¡c metrics analytics

**ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t!**

#### 3.1. Join cÃ¡c báº£ng

**Váº¥n Ä‘á»**: Dá»¯ liá»‡u náº±m ráº£i rÃ¡c trong nhiá»u báº£ng

**Giáº£i phÃ¡p**: Join Ä‘á»ƒ táº¡o má»™t view hoÃ n chá»‰nh

```python
sales_complete = order_detail \
    .join(order_header, "SalesOrderID", "inner") \
    .join(product, order_detail.ProductID == product.ProductID, "left") \
    .join(subcategory, product.ProductSubcategoryID == subcategory.ProductSubcategoryID, "left") \
    .join(category, subcategory.ProductCategoryID == category.ProductCategoryID, "left")
```

**Giáº£i thÃ­ch tá»«ng join**:

1. **OrderDetail JOIN OrderHeader**:
   - Má»¥c Ä‘Ã­ch: Láº¥y thÃ´ng tin Ä‘Æ¡n hÃ ng (OrderDate, CustomerID, TotalDue)
   - Key: `SalesOrderID`
   - Type: `inner` (chá»‰ láº¥y orders cÃ³ cáº£ header vÃ  detail)

2. **JOIN Product**:
   - Má»¥c Ä‘Ã­ch: Láº¥y tÃªn sáº£n pháº©m
   - Key: `ProductID`
   - Type: `left` (giá»¯ láº¡i cáº£ products khÃ´ng cÃ³ trong orders)

3. **JOIN Subcategory**:
   - Má»¥c Ä‘Ã­ch: Láº¥y subcategory name
   - Key: `ProductSubcategoryID`
   - Type: `left`

4. **JOIN Category**:
   - Má»¥c Ä‘Ã­ch: Láº¥y category name (Bikes, Components, Clothing, Accessories)
   - Key: `ProductCategoryID`
   - Type: `left`

**Káº¿t quáº£**: Má»™t DataFrame cÃ³ Ä‘áº§y Ä‘á»§ thÃ´ng tin:
```
SalesOrderID | OrderDate | ProductName | CategoryName | LineTotal | ...
```

#### 3.2. Táº¡o Analytics Tables

Pipeline táº¡o 3 analytics tables:

##### A. Sales by Category and Year

```python
sales_by_category_year = sales_complete \
    .withColumn("Year", year("OrderDate")) \
    .withColumn("Month", month("OrderDate")) \
    .groupBy("Year", "Month", "CategoryName") \
    .agg(
        sum("LineTotal").alias("TotalRevenue"),
        count("*").alias("OrderLineCount"),
        avg("LineTotal").alias("AvgLineTotal"),
        sum("OrderQty").alias("TotalQuantity")
    )
```

**Má»¥c Ä‘Ã­ch**: Xem doanh thu theo tá»«ng category, nÄƒm, thÃ¡ng

**Káº¿t quáº£**:
```
Year | Month | CategoryName | TotalRevenue | OrderLineCount | ...
2021 | 1     | Bikes        | 150000.00    | 500            | ...
2021 | 1     | Components   | 80000.00     | 300            | ...
```

##### B. Top Products

```python
top_products = sales_complete \
    .groupBy("ProductName", "CategoryName") \
    .agg(
        sum("LineTotal").alias("TotalRevenue"),
        sum("OrderQty").alias("TotalQuantity"),
        count("*").alias("OrderCount"),
        avg("UnitPrice").alias("AvgUnitPrice")
    ) \
    .orderBy(desc("TotalRevenue")) \
    .limit(50)
```

**Má»¥c Ä‘Ã­ch**: TÃ¬m top 50 sáº£n pháº©m bÃ¡n cháº¡y nháº¥t

**Káº¿t quáº£**:
```
ProductName        | CategoryName | TotalRevenue | TotalQuantity | ...
Mountain-200       | Bikes        | 500000.00    | 200           | ...
Road-250           | Bikes        | 450000.00    | 180           | ...
```

##### C. Sales by Year Summary

```python
sales_by_year = order_header \
    .withColumn("Year", year("OrderDate")) \
    .groupBy("Year") \
    .agg(
        sum("TotalDue").alias("TotalRevenue"),
        avg("TotalDue").alias("AvgOrderValue"),
        count("*").alias("OrderCount"),
        countDistinct("CustomerID").alias("UniqueCustomers")
    )
```

**Má»¥c Ä‘Ã­ch**: Tá»•ng quan doanh thu theo nÄƒm

**Káº¿t quáº£**:
```
Year | TotalRevenue | AvgOrderValue | OrderCount | UniqueCustomers
2021 | 5000000.00   | 1500.00       | 3333       | 2000
2022 | 6000000.00   | 1600.00       | 3750       | 2200
```

---

### 4. Function `load_data()`

**Má»¥c Ä‘Ã­ch**: LÆ°u analytics data vÃ o Data Lake (Parquet format)

**CÃ¡ch hoáº¡t Ä‘á»™ng**:

```python
def load_data(analytics: dict, config: dict, logger, spark: SparkSession):
```

**Quy trÃ¬nh**:

1. **Láº¥y Ä‘Æ°á»ng dáº«n** tá»« config:
   ```python
   hdfs_analytics = get_hdfs_path(config, 'analytics')
   # "file:///D:/.../data_lake/adw/analytics"
   ```

2. **Ghi tá»«ng analytics table**:
   ```python
   for table_name, df in analytics.items():
       output_path = f"{hdfs_analytics}/{table_name}"
       df.write.mode("overwrite").parquet(output_path)
   ```

3. **Verify**: Äá»c láº¡i Ä‘á»ƒ Ä‘áº£m báº£o ghi thÃ nh cÃ´ng
   ```python
   verify_df = spark.read.parquet(output_path)
   logger.info(f"Written {verify_df.count()} rows")
   ```

**Táº¡i sao dÃ¹ng Parquet?**
- **Columnar format**: Äá»c nhanh hÆ¡n CSV
- **Compressed**: Tiáº¿t kiá»‡m dung lÆ°á»£ng
- **Schema**: Giá»¯ Ä‘Æ°á»£c kiá»ƒu dá»¯ liá»‡u
- **Spark native**: Spark Ä‘á»c Parquet ráº¥t nhanh

**Káº¿t quáº£**: Files trong `data_lake/adw/analytics/`:
```
adw/analytics/
â”œâ”€â”€ sales_by_year/
â”‚   â””â”€â”€ part-00000-xxx.parquet
â”œâ”€â”€ top_products/
â”‚   â””â”€â”€ part-00000-xxx.parquet
â””â”€â”€ sales_by_category_year/
    â””â”€â”€ part-00000-xxx.parquet
```

---

### 5. Function `main()`

**Má»¥c Ä‘Ã­ch**: Orchestrate toÃ n bá»™ pipeline

**Flow**:

```python
def main():
    # 1. Setup
    config = load_config()           # Load config tá»« YAML
    logger = setup_logging(config)   # Setup logging
    spark = create_spark_session(config)  # Táº¡o Spark session
    
    try:
        # 2. Extract
        dataframes = extract_data(spark, config, logger)
        
        # 3. Validate
        validate_data(dataframes, logger)
        
        # 4. Transform
        analytics = transform_data(dataframes, logger)
        
        # 5. Load
        load_data(analytics, config, logger, spark)
        
        # 6. Show results
        analytics['sales_by_year'].show(10)
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise
    finally:
        spark.stop()  # ÄÃ³ng Spark session
```

**Táº¡i sao cÃ³ try-except-finally?**
- **try**: Cháº¡y pipeline
- **except**: Báº¯t lá»—i vÃ  log
- **finally**: Äáº£m báº£o Spark session luÃ´n Ä‘Æ°á»£c Ä‘Ã³ng (dÃ¹ thÃ nh cÃ´ng hay tháº¥t báº¡i)

---

## ğŸ”„ Luá»“ng Dá»¯ Liá»‡u Chi Tiáº¿t

### VÃ­ dá»¥ vá»›i 1 Ä‘Æ¡n hÃ ng:

**BÆ°á»›c 1: Extract**
```
SQL Server:
â”œâ”€â”€ SalesOrderHeader: {SalesOrderID: 43659, OrderDate: 2011-05-31, TotalDue: 23153.23}
â””â”€â”€ SalesOrderDetail: {SalesOrderDetailID: 1, SalesOrderID: 43659, ProductID: 776, LineTotal: 2024.99}
```

**BÆ°á»›c 2: Join**
```
sales_complete:
SalesOrderID: 43659
OrderDate: 2011-05-31
ProductID: 776
ProductName: "Mountain-200 Silver, 38"
CategoryName: "Bikes"
LineTotal: 2024.99
```

**BÆ°á»›c 3: Aggregate**
```
sales_by_category_year:
Year: 2011
Month: 5
CategoryName: "Bikes"
TotalRevenue: 1500000.00 (tá»•ng táº¥t cáº£ Bikes trong thÃ¡ng 5/2011)
```

**BÆ°á»›c 4: Load**
```
Parquet file: data_lake/adw/analytics/sales_by_category_year/part-00000.parquet
```

---

## ğŸ’¡ Táº¡i Sao Cáº§n Pipeline NÃ y?

### Váº¥n Ä‘á» náº¿u khÃ´ng cÃ³ pipeline:

1. **Query trá»±c tiáº¿p tá»« SQL Server**:
   ```sql
   SELECT c.Name, YEAR(h.OrderDate), SUM(d.LineTotal)
   FROM SalesOrderHeader h
   JOIN SalesOrderDetail d ON h.SalesOrderID = d.SalesOrderID
   JOIN Product p ON d.ProductID = p.ProductID
   JOIN ProductSubcategory s ON p.ProductSubcategoryID = s.ProductSubcategoryID
   JOIN ProductCategory c ON s.ProductCategoryID = c.ProductCategoryID
   GROUP BY c.Name, YEAR(h.OrderDate)
   ```
   - Cháº­m vá»›i dá»¯ liá»‡u lá»›n
   - Pháº£i query láº¡i má»—i láº§n cáº§n
   - LÃ m cháº­m SQL Server

2. **KhÃ´ng cÃ³ data lake**:
   - KhÃ´ng thá»ƒ lÆ°u trá»¯ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
   - Pháº£i tÃ­nh toÃ¡n láº¡i má»—i láº§n

### Giáº£i phÃ¡p vá»›i Pipeline:

1. **Extract má»™t láº§n**: Láº¥y data tá»« SQL Server má»™t láº§n
2. **Transform**: TÃ­nh toÃ¡n metrics má»™t láº§n
3. **Load**: LÆ°u vÃ o data lake (Parquet)
4. **Reuse**: CÃ³ thá»ƒ Ä‘á»c láº¡i nhiá»u láº§n mÃ  khÃ´ng cáº§n tÃ­nh láº¡i

**Lá»£i Ã­ch**:
- âš¡ Nhanh hÆ¡n: Parquet Ä‘á»c nhanh hÆ¡n SQL queries
- ğŸ’° Tiáº¿t kiá»‡m: KhÃ´ng pháº£i query SQL Server nhiá»u láº§n
- ğŸ“Š Sáºµn sÃ ng: Data Ä‘Ã£ sáºµn sÃ ng cho analytics
- ğŸ”„ TÃ¡i sá»­ dá»¥ng: CÃ³ thá»ƒ dÃ¹ng cho nhiá»u reports khÃ¡c nhau

---

## ğŸ“ TÃ³m Táº¯t

**Pipeline nÃ y lÃ m gÃ¬?**
1. Láº¥y dá»¯ liá»‡u tá»« SQL Server
2. Join cÃ¡c báº£ng láº¡i vá»›i nhau
3. TÃ­nh toÃ¡n cÃ¡c metrics analytics
4. LÆ°u vÃ o data lake (Parquet)

**Káº¿t quáº£**:
- 3 analytics tables sáºµn sÃ ng Ä‘á»ƒ query
- Data Ä‘Æ°á»£c lÆ°u trong Parquet format (nhanh, compressed)
- CÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ táº¡o reports, dashboards, v.v.

**Khi nÃ o cháº¡y?**
- Cháº¡y Ä‘á»‹nh ká»³ (daily/weekly) Ä‘á»ƒ cáº­p nháº­t analytics
- Hoáº·c cháº¡y má»™t láº§n Ä‘á»ƒ táº¡o data lake ban Ä‘áº§u

---

## ğŸ“ Äiá»ƒm Há»c Táº­p

1. **ETL Pattern**: Extract â†’ Transform â†’ Load
2. **Spark Joins**: Inner join, Left join
3. **Aggregations**: GroupBy, Sum, Count, Avg
4. **Data Lake**: LÆ°u trá»¯ dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
5. **Parquet Format**: Columnar storage cho analytics
6. **Error Handling**: Try-except-finally
7. **Logging**: Ghi log Ä‘á»ƒ theo dÃµi pipeline

---

**Hy vá»ng giáº£i thÃ­ch nÃ y giÃºp báº¡n hiá»ƒu rÃµ pipeline! ğŸš€**

