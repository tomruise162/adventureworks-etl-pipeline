# ğŸ“– HÆ°á»›ng dáº«n thá»±c hÃ nh vá»›i AdventureWorks2022
## Ãp dá»¥ng Roadmap vÃ o Database thá»±c táº¿

---

## ğŸ¯ Táº¡i sao AdventureWorks2022 phÃ¹ há»£p?

AdventureWorks2022 lÃ  database máº«u cá»§a Microsoft vá»›i:
- âœ… **Dá»¯ liá»‡u Ä‘a dáº¡ng**: Sales, Production, Human Resources, Purchasing
- âœ… **Quan há»‡ phá»©c táº¡p**: Nhiá»u báº£ng vá»›i relationships phá»©c táº¡p
- âœ… **Dá»¯ liá»‡u lá»›n**: HÃ ng trÄƒm nghÃ¬n records
- âœ… **Thá»±c táº¿**: Giá»‘ng vá»›i database production tháº­t
- âœ… **Miá»…n phÃ­**: Microsoft cung cáº¥p free

---

## ğŸ“Š Cáº¥u trÃºc Database AdventureWorks2022

### Schema chÃ­nh:
- **Sales**: SalesOrderHeader, SalesOrderDetail, Customer, Store
- **Production**: Product, ProductCategory, ProductSubcategory, Inventory
- **HumanResources**: Employee, Department, Shift
- **Purchasing**: PurchaseOrderHeader, PurchaseOrderDetail, Vendor

### Báº£ng quan trá»ng cho Data Engineering:

#### Sales Schema
```
Sales.SalesOrderHeader (31,465 rows)
â”œâ”€â”€ SalesOrderID (PK)
â”œâ”€â”€ OrderDate
â”œâ”€â”€ CustomerID
â”œâ”€â”€ SalesPersonID
â”œâ”€â”€ TotalDue
â””â”€â”€ ...

Sales.SalesOrderDetail (121,317 rows)
â”œâ”€â”€ SalesOrderDetailID (PK)
â”œâ”€â”€ SalesOrderID (FK)
â”œâ”€â”€ ProductID (FK)
â”œâ”€â”€ OrderQty
â”œâ”€â”€ UnitPrice
â””â”€â”€ LineTotal

Sales.Customer (19,820 rows)
â”œâ”€â”€ CustomerID (PK)
â”œâ”€â”€ PersonID
â””â”€â”€ StoreID
```

#### Production Schema
```
Production.Product (504 rows)
â”œâ”€â”€ ProductID (PK)
â”œâ”€â”€ Name
â”œâ”€â”€ ProductNumber
â”œâ”€â”€ ProductSubcategoryID (FK)
â”œâ”€â”€ StandardCost
â””â”€â”€ ListPrice

Production.ProductSubcategory (37 rows)
â”œâ”€â”€ ProductSubcategoryID (PK)
â”œâ”€â”€ Name
â”œâ”€â”€ ProductCategoryID (FK)
â””â”€â”€ ...

Production.ProductCategory (4 rows)
â”œâ”€â”€ ProductCategoryID (PK)
â”œâ”€â”€ Name
â””â”€â”€ ...
```

---

## ğŸ—ºï¸ Ãp dá»¥ng Roadmap vá»›i AdventureWorks2022

### Giai Ä‘oáº¡n 1: Ná»n táº£ng

#### BÃ i táº­p 1.1: SQL Fundamentals vá»›i AdventureWorks
```sql
-- 1. Basic SELECT
SELECT TOP 10 * FROM Sales.SalesOrderHeader;

-- 2. WHERE clause
SELECT * FROM Sales.SalesOrderHeader 
WHERE OrderDate >= '2013-01-01';

-- 3. JOINs
SELECT 
    h.SalesOrderID,
    h.OrderDate,
    h.TotalDue,
    c.CustomerID
FROM Sales.SalesOrderHeader h
INNER JOIN Sales.Customer c ON h.CustomerID = c.CustomerID;

-- 4. Aggregations
SELECT 
    YEAR(OrderDate) AS Year,
    COUNT(*) AS OrderCount,
    SUM(TotalDue) AS TotalRevenue
FROM Sales.SalesOrderHeader
GROUP BY YEAR(OrderDate)
ORDER BY Year;

-- 5. Window Functions
SELECT 
    SalesOrderID,
    OrderDate,
    TotalDue,
    ROW_NUMBER() OVER (PARTITION BY YEAR(OrderDate) ORDER BY TotalDue DESC) AS Rank
FROM Sales.SalesOrderHeader;
```

**Má»¥c tiÃªu**: LÃ m quen vá»›i database vÃ  viáº¿t queries phá»©c táº¡p

---

### Giai Ä‘oáº¡n 2: Hadoop Ecosystem

#### BÃ i táº­p 2.1: Export data tá»« SQL Server
```bash
# Export SalesOrderHeader ra CSV
bcp "SELECT * FROM AdventureWorks2022.Sales.SalesOrderHeader" queryout sales_order_header.csv -c -T -S localhost

# Export Product ra CSV
bcp "SELECT * FROM AdventureWorks2022.Production.Product" queryout product.csv -c -T -S localhost
```

#### BÃ i táº­p 2.2: Upload lÃªn HDFS
```bash
# Táº¡o directory structure
hdfs dfs -mkdir -p /adw/raw/sales
hdfs dfs -mkdir -p /adw/raw/production

# Upload files
hdfs dfs -put sales_order_header.csv /adw/raw/sales/
hdfs dfs -put product.csv /adw/raw/production/

# Kiá»ƒm tra
hdfs dfs -ls -R /adw
hdfs dfs -du -h /adw/raw/sales/
```

#### BÃ i táº­p 2.3: HDFS Operations
```bash
# Xem file content
hdfs dfs -cat /adw/raw/sales/sales_order_header.csv | head -20

# Copy file
hdfs dfs -cp /adw/raw/sales/sales_order_header.csv /adw/backup/

# Set permissions
hdfs dfs -chmod 755 /adw/raw/sales/
hdfs dfs -chown hdfs:hdfs /adw/raw/sales/

# Check replication
hdfs dfs -ls /adw/raw/sales/sales_order_header.csv
```

**Má»¥c tiÃªu**: ThÃ nh tháº¡o HDFS commands vá»›i dá»¯ liá»‡u tháº­t

---

### Giai Ä‘oáº¡n 3: Spark Core (RDD)

#### BÃ i táº­p 3.1: Äá»c dá»¯ liá»‡u tá»« HDFS
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("AdventureWorksRDD") \
    .getOrCreate()

sc = spark.sparkContext

# Äá»c file tá»« HDFS
sales_rdd = sc.textFile("hdfs://localhost:9000/adw/raw/sales/sales_order_header.csv")

# Xem sá»‘ partitions
print(f"Number of partitions: {sales_rdd.getNumPartitions()}")

# Xem vÃ i dÃ²ng Ä‘áº§u
sales_rdd.take(5)
```

#### BÃ i táº­p 3.2: Transformations vá»›i RDD
```python
# Parse CSV (bá» header)
header = sales_rdd.first()
sales_data = sales_rdd.filter(lambda line: line != header)

# Map: Extract OrderDate vÃ  TotalDue
def parse_line(line):
    parts = line.split(',')
    try:
        order_date = parts[2]  # Giáº£ sá»­ OrderDate á»Ÿ cá»™t 3
        total_due = float(parts[20])  # Giáº£ sá»­ TotalDue á»Ÿ cá»™t 21
        return (order_date, total_due)
    except:
        return None

sales_parsed = sales_data.map(parse_line).filter(lambda x: x is not None)

# Filter: Chá»‰ láº¥y orders nÄƒm 2013
sales_2013 = sales_parsed.filter(lambda x: x[0].startswith('2013'))

# Reduce: TÃ­nh tá»•ng doanh thu
total_revenue = sales_2013.map(lambda x: x[1]).reduce(lambda a, b: a + b)
print(f"Total Revenue 2013: {total_revenue}")
```

#### BÃ i táº­p 3.3: GroupBy vÃ  Aggregations
```python
# Group by Year
def extract_year(date_total):
    date, total = date_total
    year = date.split('-')[0]
    return (year, total)

sales_by_year = sales_parsed.map(extract_year) \
    .groupByKey() \
    .mapValues(lambda values: sum(values))

sales_by_year.collect()
```

#### BÃ i táº­p 3.4: Join RDDs
```python
# Äá»c Product RDD
product_rdd = sc.textFile("hdfs://localhost:9000/adw/raw/production/product.csv")
product_header = product_rdd.first()
product_data = product_rdd.filter(lambda line: line != header)

# Parse Product: (ProductID, Name)
def parse_product(line):
    parts = line.split(',')
    return (parts[0], parts[1])  # ProductID, Name

products = product_data.map(parse_product)

# Äá»c SalesOrderDetail
detail_rdd = sc.textFile("hdfs://localhost:9000/adw/raw/sales/sales_order_detail.csv")
detail_header = detail_rdd.first()
detail_data = detail_rdd.filter(lambda line: line != header)

# Parse Detail: (ProductID, OrderQty)
def parse_detail(line):
    parts = line.split(',')
    return (parts[4], int(parts[3]))  # ProductID, OrderQty

details = detail_data.map(parse_detail)

# Join vÃ  tÃ­nh tá»•ng quantity theo product
product_sales = details.join(products) \
    .map(lambda x: (x[1][1], x[1][0])) \
    .reduceByKey(lambda a, b: a + b)

product_sales.take(10)
```

#### BÃ i táº­p 3.5: Caching
```python
# Cache RDD Ä‘Æ°á»£c dÃ¹ng nhiá»u láº§n
sales_parsed.cache()

# Láº§n 1: TÃ­nh tá»•ng
total1 = sales_parsed.map(lambda x: x[1]).reduce(lambda a, b: a + b)

# Láº§n 2: TÃ­nh trung bÃ¬nh (sáº½ nhanh hÆ¡n vÃ¬ Ä‘Ã£ cache)
avg = sales_parsed.map(lambda x: x[1]).mean()

# Unpersist khi khÃ´ng dÃ¹ng ná»¯a
sales_parsed.unpersist()
```

**Má»¥c tiÃªu**: ThÃ nh tháº¡o RDD transformations vá»›i dá»¯ liá»‡u tháº­t

---

### Giai Ä‘oáº¡n 4: Spark SQL & DataFrame

#### BÃ i táº­p 4.1: Äá»c tá»« JDBC
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("AdventureWorksDF") \
    .config("spark.jars", "file:///D:/DE_project/sqljdbc_12.10/enu/jars/mssql-jdbc-12.10.1.jre8.jar") \
    .getOrCreate()

# Káº¿t ná»‘i JDBC
jdbc_url = "jdbc:sqlserver://localhost:1433;databaseName=AdventureWorks2022;encrypt=true;trustServerCertificate=true"
props = {
    "user": "sa",
    "password": "123456",
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# Äá»c cÃ¡c báº£ng
order_header = spark.read.jdbc(jdbc_url, "Sales.SalesOrderHeader", properties=props)
order_detail = spark.read.jdbc(jdbc_url, "Sales.SalesOrderDetail", properties=props)
product = spark.read.jdbc(jdbc_url, "Production.Product", properties=props)
subcategory = spark.read.jdbc(jdbc_url, "Production.ProductSubcategory", properties=props)
category = spark.read.jdbc(jdbc_url, "Production.ProductCategory", properties=props)

# Xem schema
order_header.printSchema()
order_header.show(5)
```

#### BÃ i táº­p 4.2: DataFrame Operations
```python
from pyspark.sql.functions import col, year, sum, avg, count, when

# Select columns
order_header.select("SalesOrderID", "OrderDate", "TotalDue").show(5)

# Filter
high_value_orders = order_header.filter(col("TotalDue") > 10000)
high_value_orders.show()

# Add column
from pyspark.sql.functions import year, month
orders_with_year = order_header.withColumn("Year", year("OrderDate")) \
    .withColumn("Month", month("OrderDate"))

orders_with_year.select("SalesOrderID", "OrderDate", "Year", "Month", "TotalDue").show()
```

#### BÃ i táº­p 4.3: Aggregations
```python
# Sales by Year
sales_by_year = order_header \
    .withColumn("Year", year("OrderDate")) \
    .groupBy("Year") \
    .agg(
        sum("TotalDue").alias("TotalRevenue"),
        avg("TotalDue").alias("AvgOrderValue"),
        count("*").alias("OrderCount")
    ) \
    .orderBy("Year")

sales_by_year.show()

# Top 10 Customers
from pyspark.sql.functions import desc
top_customers = order_header \
    .groupBy("CustomerID") \
    .agg(
        sum("TotalDue").alias("TotalSpent"),
        count("*").alias("OrderCount")
    ) \
    .orderBy(desc("TotalSpent")) \
    .limit(10)

top_customers.show()
```

#### BÃ i táº­p 4.4: Joins
```python
# Join OrderHeader vá»›i OrderDetail
order_joined = order_header.join(
    order_detail,
    order_header.SalesOrderID == order_detail.SalesOrderID,
    "inner"
)

# Join vá»›i Product Ä‘á»ƒ láº¥y product name
order_with_product = order_joined.join(
    product,
    order_detail.ProductID == product.ProductID,
    "left"
)

# Join vá»›i Subcategory vÃ  Category
order_full = order_with_product \
    .join(subcategory, product.ProductSubcategoryID == subcategory.ProductSubcategoryID, "left") \
    .join(category, subcategory.ProductCategoryID == category.ProductCategoryID, "left")

order_full.select(
    "SalesOrderID",
    "OrderDate",
    "Product.Name",
    "Category.Name",
    "OrderQty",
    "LineTotal"
).show(10)
```

#### BÃ i táº­p 4.5: Spark SQL
```python
# Register temp views
order_header.createOrReplaceTempView("order_header")
order_detail.createOrReplaceTempView("order_detail")
product.createOrReplaceTempView("product")
subcategory.createOrReplaceTempView("subcategory")
category.createOrReplaceTempView("category")

# SQL Query: Sales by Category
sales_by_category = spark.sql("""
    SELECT 
        c.Name AS CategoryName,
        YEAR(h.OrderDate) AS Year,
        SUM(d.LineTotal) AS TotalRevenue,
        COUNT(DISTINCT h.SalesOrderID) AS OrderCount,
        AVG(d.LineTotal) AS AvgLineTotal
    FROM order_header h
    INNER JOIN order_detail d ON h.SalesOrderID = d.SalesOrderID
    INNER JOIN product p ON d.ProductID = p.ProductID
    LEFT JOIN subcategory s ON p.ProductSubcategoryID = s.ProductSubcategoryID
    LEFT JOIN category c ON s.ProductCategoryID = c.ProductCategoryID
    WHERE c.Name IS NOT NULL
    GROUP BY c.Name, YEAR(h.OrderDate)
    ORDER BY Year, TotalRevenue DESC
""")

sales_by_category.show(50)
```

#### BÃ i táº­p 4.6: Window Functions
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, row_number, lag, lead

# Rank customers by total spending
windowSpec = Window.partitionBy("Year").orderBy(desc("TotalRevenue"))

customer_ranking = order_header \
    .withColumn("Year", year("OrderDate")) \
    .groupBy("Year", "CustomerID") \
    .agg(sum("TotalDue").alias("TotalRevenue")) \
    .withColumn("Rank", rank().over(windowSpec)) \
    .filter(col("Rank") <= 10)

customer_ranking.show(30)

# Month-over-Month growth
monthly_sales = order_header \
    .withColumn("Year", year("OrderDate")) \
    .withColumn("Month", month("OrderDate")) \
    .groupBy("Year", "Month") \
    .agg(sum("TotalDue").alias("MonthlyRevenue")) \
    .orderBy("Year", "Month")

windowSpec2 = Window.orderBy("Year", "Month")
monthly_with_growth = monthly_sales \
    .withColumn("PrevMonthRevenue", lag("MonthlyRevenue", 1).over(windowSpec2)) \
    .withColumn("MoMGrowth", 
        ((col("MonthlyRevenue") - col("PrevMonthRevenue")) / col("PrevMonthRevenue")) * 100
    )

monthly_with_growth.show(50)
```

#### BÃ i táº­p 4.7: Write to Parquet
```python
# Write to HDFS Parquet format
order_header.write \
    .mode("overwrite") \
    .parquet("hdfs://localhost:9000/adw/raw/sales_order_header")

order_detail.write \
    .mode("overwrite") \
    .parquet("hdfs://localhost:9000/adw/raw/sales_order_detail")

product.write \
    .mode("overwrite") \
    .parquet("hdfs://localhost:9000/adw/raw/product")

# Read back from Parquet
order_header_parquet = spark.read.parquet("hdfs://localhost:9000/adw/raw/sales_order_header")
order_header_parquet.show(5)

# So sÃ¡nh performance
import time

# Äá»c tá»« JDBC
start = time.time()
order_header_jdbc = spark.read.jdbc(jdbc_url, "Sales.SalesOrderHeader", properties=props)
order_header_jdbc.count()
jdbc_time = time.time() - start

# Äá»c tá»« Parquet
start = time.time()
order_header_parquet = spark.read.parquet("hdfs://localhost:9000/adw/raw/sales_order_header")
order_header_parquet.count()
parquet_time = time.time() - start

print(f"JDBC time: {jdbc_time:.2f}s")
print(f"Parquet time: {parquet_time:.2f}s")
print(f"Speedup: {jdbc_time/parquet_time:.2f}x")
```

**Má»¥c tiÃªu**: ThÃ nh tháº¡o DataFrame API vÃ  Spark SQL vá»›i dá»¯ liá»‡u tháº­t

---

### Giai Ä‘oáº¡n 5: Advanced Spark

#### BÃ i táº­p 5.1: Tá»‘i Æ°u Spark Job
```python
# Configuration tá»‘i Æ°u
spark.conf.set("spark.sql.shuffle.partitions", "200")
spark.conf.set("spark.executor.memory", "2g")
spark.conf.set("spark.executor.cores", "2")

# Repartition data
order_header_repartitioned = order_header.repartition(10, "CustomerID")

# Coalesce Ä‘á»ƒ giáº£m partitions
order_header_coalesced = order_header.coalesce(5)
```

#### BÃ i táº­p 5.2: Broadcast Join
```python
# Category vÃ  Subcategory lÃ  small tables -> dÃ¹ng broadcast
from pyspark.sql.functions import broadcast

order_with_category = order_detail \
    .join(broadcast(product), order_detail.ProductID == product.ProductID) \
    .join(broadcast(subcategory), product.ProductSubcategoryID == subcategory.ProductSubcategoryID) \
    .join(broadcast(category), subcategory.ProductCategoryID == category.ProductCategoryID)

order_with_category.explain()  # Xem execution plan
```

#### BÃ i táº­p 5.3: Bucketing
```python
# Bucket table theo CustomerID
order_header.write \
    .mode("overwrite") \
    .bucketBy(10, "CustomerID") \
    .sortBy("OrderDate") \
    .saveAsTable("order_header_bucketed")

# Äá»c bucketed table
bucketed_df = spark.table("order_header_bucketed")
```

#### BÃ i táº­p 5.4: PhÃ¢n tÃ­ch Spark UI
```python
# Cháº¡y job vÃ  phÃ¢n tÃ­ch
sales_by_category = spark.sql("""
    SELECT 
        c.Name AS CategoryName,
        YEAR(h.OrderDate) AS Year,
        SUM(d.LineTotal) AS TotalRevenue
    FROM order_header h
    INNER JOIN order_detail d ON h.SalesOrderID = d.SalesOrderID
    INNER JOIN product p ON d.ProductID = p.ProductID
    LEFT JOIN subcategory s ON p.ProductSubcategoryID = s.ProductSubcategoryID
    LEFT JOIN category c ON s.ProductCategoryID = c.ProductCategoryID
    GROUP BY c.Name, YEAR(h.OrderDate)
""")

# Xem execution plan
sales_by_category.explain(extended=True)

# Collect Ä‘á»ƒ trigger execution
result = sales_by_category.collect()

# Sau Ä‘Ã³ má»Ÿ Spark UI: http://localhost:4040
# PhÃ¢n tÃ­ch:
# - Stages vÃ  Tasks
# - Shuffle operations
# - Execution time
```

**Má»¥c tiÃªu**: Tá»‘i Æ°u Spark jobs vÃ  hiá»ƒu execution model

---

### Giai Ä‘oáº¡n 6: Data Engineering Best Practices

#### BÃ i táº­p 6.1: ETL Pipeline hoÃ n chá»‰nh
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def extract_data(spark, jdbc_url, props):
    """Extract data from SQL Server"""
    try:
        logger.info("Starting data extraction...")
        
        order_header = spark.read.jdbc(jdbc_url, "Sales.SalesOrderHeader", properties=props)
        order_detail = spark.read.jdbc(jdbc_url, "Sales.SalesOrderDetail", properties=props)
        product = spark.read.jdbc(jdbc_url, "Production.Product", properties=props)
        subcategory = spark.read.jdbc(jdbc_url, "Production.ProductSubcategory", properties=props)
        category = spark.read.jdbc(jdbc_url, "Production.ProductCategory", properties=props)
        
        logger.info(f"Extracted {order_header.count()} orders")
        return order_header, order_detail, product, subcategory, category
        
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        raise

def transform_data(order_header, order_detail, product, subcategory, category):
    """Transform and join data"""
    try:
        logger.info("Starting data transformation...")
        
        # Join all tables
        sales_analytics = order_detail \
            .join(order_header, "SalesOrderID", "inner") \
            .join(product, order_detail.ProductID == product.ProductID, "left") \
            .join(subcategory, product.ProductSubcategoryID == subcategory.ProductSubcategoryID, "left") \
            .join(category, subcategory.ProductCategoryID == category.ProductCategoryID, "left")
        
        # Create analytics table: Sales by Category and Year
        sales_by_category_year = sales_analytics \
            .withColumn("Year", year("OrderDate")) \
            .groupBy("Year", category.Name.alias("CategoryName")) \
            .agg(
                sum("LineTotal").alias("TotalRevenue"),
                count("*").alias("OrderLineCount"),
                avg("LineTotal").alias("AvgLineTotal")
            ) \
            .filter(col("CategoryName").isNotNull()) \
            .orderBy("Year", desc("TotalRevenue"))
        
        logger.info("Data transformation completed")
        return sales_by_category_year
        
    except Exception as e:
        logger.error(f"Error transforming data: {e}")
        raise

def load_data(df, output_path):
    """Load data to HDFS"""
    try:
        logger.info(f"Loading data to {output_path}...")
        
        df.write \
            .mode("overwrite") \
            .parquet(output_path)
        
        logger.info("Data loaded successfully")
        
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        raise

# Main ETL pipeline
def run_etl():
    spark = SparkSession.builder \
        .appName("AdventureWorksETL") \
        .config("spark.jars", "file:///D:/DE_project/sqljdbc_12.10/enu/jars/mssql-jdbc-12.10.1.jre8.jar") \
        .getOrCreate()
    
    jdbc_url = "jdbc:sqlserver://localhost:1433;databaseName=AdventureWorks2022;encrypt=true;trustServerCertificate=true"
    props = {
        "user": "sa",
        "password": "123456",
        "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
    }
    
    try:
        # Extract
        order_header, order_detail, product, subcategory, category = extract_data(spark, jdbc_url, props)
        
        # Transform
        sales_analytics = transform_data(order_header, order_detail, product, subcategory, category)
        
        # Load
        load_data(sales_analytics, "hdfs://localhost:9000/adw/analytics/sales_by_category_year")
        
        logger.info("ETL pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"ETL pipeline failed: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    run_etl()
```

#### BÃ i táº­p 6.2: Data Quality Checks
```python
def validate_data(df, table_name):
    """Validate data quality"""
    logger.info(f"Validating {table_name}...")
    
    # Check 1: Row count
    row_count = df.count()
    logger.info(f"{table_name} row count: {row_count}")
    
    if row_count == 0:
        raise ValueError(f"{table_name} is empty!")
    
    # Check 2: Null values in key columns
    null_counts = {}
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        if null_count > 0:
            null_counts[col_name] = null_count
    
    if null_counts:
        logger.warning(f"Null values found in {table_name}: {null_counts}")
    
    # Check 3: Duplicate check
    duplicate_count = df.count() - df.distinct().count()
    if duplicate_count > 0:
        logger.warning(f"Found {duplicate_count} duplicate rows in {table_name}")
    
    # Check 4: Data range validation
    if "TotalDue" in df.columns:
        negative_values = df.filter(col("TotalDue") < 0).count()
        if negative_values > 0:
            logger.warning(f"Found {negative_values} negative TotalDue values")
    
    logger.info(f"{table_name} validation completed")
    return True

# Sá»­ dá»¥ng
validate_data(order_header, "order_header")
validate_data(order_detail, "order_detail")
```

#### BÃ i táº­p 6.3: Incremental Load
```python
def incremental_load(spark, jdbc_url, props, last_load_date):
    """Load only new/updated records"""
    
    # Read only new orders
    query = f"""
        (SELECT * FROM Sales.SalesOrderHeader 
         WHERE ModifiedDate > '{last_load_date}') AS new_orders
    """
    
    new_orders = spark.read.jdbc(
        jdbc_url,
        query,
        properties=props
    )
    
    # Read existing data
    existing_orders = spark.read.parquet("hdfs://localhost:9000/adw/raw/sales_order_header")
    
    # Merge: Union new orders with existing
    all_orders = existing_orders.union(new_orders)
    
    # Remove duplicates (keep latest)
    all_orders_dedup = all_orders \
        .orderBy("ModifiedDate", ascending=False) \
        .dropDuplicates(["SalesOrderID"])
    
    # Write back
    all_orders_dedup.write \
        .mode("overwrite") \
        .parquet("hdfs://localhost:9000/adw/raw/sales_order_header")
    
    return all_orders_dedup.count()
```

**Má»¥c tiÃªu**: XÃ¢y dá»±ng production-ready ETL pipeline

---

## ğŸ“ Cáº¥u trÃºc Project Ä‘á» xuáº¥t

```
SalesCategoryAnalytics/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_data.py          # Extract tá»« SQL Server
â”‚   â”œâ”€â”€ transform_data.py        # Transform data
â”‚   â”œâ”€â”€ load_data.py             # Load vÃ o HDFS
â”‚   â”œâ”€â”€ etl_pipeline.py          # Full ETL pipeline
â”‚   â””â”€â”€ data_quality.py          # Data quality checks
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_sql_practice.ipynb    # SQL exercises
â”‚   â”œâ”€â”€ 02_rdd_practice.ipynb    # RDD exercises
â”‚   â”œâ”€â”€ 03_dataframe_practice.ipynb  # DataFrame exercises
â”‚   â”œâ”€â”€ 04_analytics.ipynb       # Analytics queries
â”‚   â””â”€â”€ 05_optimization.ipynb    # Performance tuning
â”œâ”€â”€ data_lake/
â”‚   â””â”€â”€ (sáº½ Ä‘Æ°á»£c táº¡o khi cháº¡y pipeline)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ (sáº½ Ä‘Æ°á»£c táº¡o khi cháº¡y pipeline)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Configuration file
â”œâ”€â”€ ROADMAP.md                   # Roadmap há»c táº­p
â”œâ”€â”€ PRACTICE_GUIDE.md            # File nÃ y
â””â”€â”€ README.md                    # Project documentation
```

---

## âœ… Checklist thá»±c hÃ nh

### Giai Ä‘oáº¡n 1
- [ ] Viáº¿t 10 SQL queries phá»©c táº¡p vá»›i AdventureWorks
- [ ] Hiá»ƒu Ä‘Æ°á»£c relationships giá»¯a cÃ¡c báº£ng
- [ ] Viáº¿t window functions queries

### Giai Ä‘oáº¡n 2
- [ ] Export data tá»« SQL Server ra CSV
- [ ] Upload lÃªn HDFS vÃ  táº¡o directory structure
- [ ] Thá»±c hÃ nh táº¥t cáº£ HDFS commands

### Giai Ä‘oáº¡n 3
- [ ] Äá»c data tá»« HDFS báº±ng RDD
- [ ] Viáº¿t transformations phá»©c táº¡p
- [ ] Join nhiá»u RDDs
- [ ] Sá»­ dá»¥ng caching

### Giai Ä‘oáº¡n 4
- [ ] Äá»c data tá»« JDBC vÃ o DataFrame
- [ ] Viáº¿t DataFrame operations
- [ ] Viáº¿t Spark SQL queries
- [ ] Write/Read Parquet files
- [ ] So sÃ¡nh performance JDBC vs Parquet

### Giai Ä‘oáº¡n 5
- [ ] Tá»‘i Æ°u Spark jobs
- [ ] Sá»­ dá»¥ng broadcast joins
- [ ] PhÃ¢n tÃ­ch Spark UI
- [ ] Tá»‘i Æ°u partitioning

### Giai Ä‘oáº¡n 6
- [ ] XÃ¢y dá»±ng ETL pipeline hoÃ n chá»‰nh
- [ ] Implement data quality checks
- [ ] Implement incremental load
- [ ] Add logging vÃ  error handling

---

## ğŸ¯ Dá»± Ã¡n Portfolio vá»›i AdventureWorks

### Project 1: Sales Analytics Dashboard
- Extract: Tá»« SQL Server
- Transform: Join cÃ¡c báº£ng, tÃ­nh metrics
- Load: VÃ o HDFS Parquet
- Analytics: Sales by Category, Year, Customer
- Output: Parquet files cho reporting

### Project 2: Customer Segmentation
- PhÃ¢n tÃ­ch customer behavior
- RFM Analysis (Recency, Frequency, Monetary)
- Clustering customers
- Output: Customer segments

### Project 3: Product Performance Analysis
- Top products by revenue
- Product category analysis
- Inventory analysis
- Output: Product performance metrics

---

## ğŸ’¡ Tips thá»±c hÃ nh

1. **Báº¯t Ä‘áº§u Ä‘Æ¡n giáº£n**: Tá»« SQL queries â†’ RDD â†’ DataFrame
2. **So sÃ¡nh performance**: JDBC vs Parquet, RDD vs DataFrame
3. **PhÃ¢n tÃ­ch Spark UI**: Hiá»ƒu execution plan
4. **Document code**: Comment vÃ  document má»i bÆ°á»›c
5. **Version control**: Commit code lÃªn GitHub
6. **Thá»­ nghiá»‡m**: Thá»­ cÃ¡c cÃ¡ch khÃ¡c nhau Ä‘á»ƒ giáº£i quyáº¿t cÃ¹ng má»™t váº¥n Ä‘á»

---

## ğŸš€ Next Steps

1. **Báº¯t Ä‘áº§u vá»›i SQL**: LÃ m quen vá»›i AdventureWorks database
2. **Setup HDFS**: CÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh HDFS local
3. **Practice daily**: LÃ m bÃ i táº­p má»—i ngÃ y
4. **Build project**: XÃ¢y dá»±ng ETL pipeline hoÃ n chá»‰nh

**ChÃºc báº¡n há»c táº­p tá»‘t vá»›i AdventureWorks2022! ğŸ“**

